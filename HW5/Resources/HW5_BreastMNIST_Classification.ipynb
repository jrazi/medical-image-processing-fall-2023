{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# HW5 - Binary Classification using Swin Transformer\n",
        "Intelligent Analysis of Biomedical Images\n",
        "\n",
        "Fall 2023"
      ],
      "metadata": {
        "id": "ssH9Unh4YeED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Name:\n",
        "- Student id:"
      ],
      "metadata": {
        "id": "Pq8Lg7Pk33_H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction**:\n",
        "\n",
        "In this educational notebook, we will explore binary classification on the BreastMNIST dataset, consisting of 780 breast ultrasound images. This task involves using the Swin Transformer, a cutting-edge neural network model, to distinguish between benign (including normal) and malignant cases.\n",
        "\n",
        "We'll tackle the common challenge of class imbalance in medical datasets and learn to improve model performance using class weights. Additionally, we'll delve into evaluating our model with ROC curves and AUC, essential tools for assessing performance in medical image classification.\n"
      ],
      "metadata": {
        "id": "Omg61bVMZs5y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db3POtrjRyom"
      },
      "source": [
        "## Packages & Modules"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "from torchmetrics import Accuracy, F1Score, ConfusionMatrix\n",
        "\n",
        "!pip install torchmetrics\n",
        "!pip install timm\n",
        "!pip install medmnist\n",
        "\n",
        "from torchmetrics import Accuracy\n",
        "from timm import create_model\n",
        "import medmnist\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "ed8uzVmiKNRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "dP31J_XYAOC5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The BreastMNIST dataset is simplified into a binary classification problem, merging normal and benign images into a single class, contrasting against malignant images. The dataset, originally 1×500×500 in size, is resized to 1×28×28 and split into training, validation, and test sets. We address the class imbalance by applying class weights in our loss function, enhancing the focus on underrepresented classes."
      ],
      "metadata": {
        "id": "RC5alm9Htg3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "info = medmnist.INFO['breastmnist']\n",
        "n_channels = info['n_channels']\n",
        "\n",
        "DataClass = getattr(medmnist, info['python_class'])"
      ],
      "metadata": {
        "id": "rH1INOxS8-iM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(info['n_samples'])\n",
        "print(info['label'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2vKXIcdXNLT",
        "outputId": "8f849a81-ae04-47b0-f532-1bb59fe96347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train': 546, 'val': 78, 'test': 156}\n",
            "{'0': 'malignant', '1': 'normal, benign'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.AugMix(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[.5], std=[.5])\n",
        "])\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[.5], std=[.5])\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# load the data\n",
        "train_dataset = DataClass(split='train', transform=train_transform, download=True)\n",
        "val_dataset = DataClass(split='val', transform=train_transform, download=True)\n",
        "test_dataset = DataClass(split='test', transform=test_transform, download=True)\n",
        "\n",
        "# Create instances of your dataset and data loader\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "TD22o8uW9L1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = next(iter(train_loader))\n",
        "images = images[:16]\n",
        "labels = labels[:16]\n",
        "\n",
        "fig, axes = plt.subplots(2, 8, figsize=(8, 4))\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    # Convert the tensor image to numpy array format\n",
        "    image = images[i].numpy().transpose(1, 2, 0)\n",
        "    # Normalize the image to the range [0, 1]\n",
        "    image = (image + 1) / 2\n",
        "    ax.imshow(image, cmap='gray')\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(f\"{info['label'][str(labels[i].item())]}\", fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3uBLJQipXgj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "FM3F2DToEepT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model(model_name=\"swin_tiny_patch4_window7_224\",\n",
        "                        pretrained=True,\n",
        "                        num_classes=1,\n",
        "                        in_chans=n_channels)\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "hVHzvai7fV3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics import Accuracy, F1Score, ConfusionMatrix\n",
        "\n",
        "def train_one_epoch(model, train_loader, loss_fn, optimizer, epoch=None):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_acc = 0.0\n",
        "\n",
        "    # Hint: Implement one training epoch using the provided model, data loader, loss function, and optimizer.\n",
        "    # Your code should include forward and backward passes, updating the model's parameters, and tracking metrics.\n",
        "    # Calculate and return the average loss, accuracy, F1-score, and confusion matrix for the epoch.\n",
        "    # Your code here [20 score]\n",
        "\n",
        "    return model, avg_loss, avg_acc, avg_f1, cm\n"
      ],
      "metadata": {
        "id": "c8MtsVBOLUGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_one_epoch(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_acc = 0.0\n",
        "\n",
        "    # Hint: Implement one validation epoch using the provided model, data loader, and loss criterion.\n",
        "    # Calculate and return the average loss, accuracy, F1-score, and confusion matrix for the epoch.\n",
        "    # Your code here [20 score]\n",
        "\n",
        "    return avg_loss, avg_acc, avg_f1, cm"
      ],
      "metadata": {
        "id": "MpMiYlRCf-PZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "n_epochs = 100\n",
        "loss_train_hist = []\n",
        "loss_valid_hist = []\n",
        "acc_train_hist = []\n",
        "acc_valid_hist = []\n",
        "f1_train_hist = []\n",
        "f1_valid_hist = []\n",
        "cm_train_hist = []\n",
        "cm_valid_hist = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    model, loss_train, acc_train, f1_train, cm_train = train_one_epoch(model, train_loader, criterion, optimizer, epoch)\n",
        "    loss_valid, acc_valid, f1_valid, cm_valid = validate_one_epoch(model, val_loader, criterion)\n",
        "\n",
        "    # Append metrics to history\n",
        "    loss_train_hist.append(loss_train)\n",
        "    loss_valid_hist.append(loss_valid)\n",
        "    acc_train_hist.append(acc_train)\n",
        "    acc_valid_hist.append(acc_valid)\n",
        "    f1_train_hist.append(f1_train)\n",
        "    f1_valid_hist.append(f1_valid)\n",
        "    cm_train_hist.append(cm_train)\n",
        "    cm_valid_hist.append(cm_valid)\n",
        "\n",
        "    print(f\"Epoch: {epoch+1}, Train Loss: {loss_train:.2f}, Train Acc: {acc_train:.2f}, Train F1: {f1_train:.2f}, Valid Loss: {loss_valid:.2f}, Valid Acc: {acc_valid:.2f}, Valid F1: {f1_valid:.2f}\")\n"
      ],
      "metadata": {
        "id": "CEuIQV5Wlpxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_performance(loss_train_hist, loss_valid_hist, acc_train_hist, acc_valid_hist):\n",
        "    plt.figure(figsize=(6, 3))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(loss_train_hist, label='Train Loss')\n",
        "    plt.plot(loss_valid_hist, label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(acc_train_hist, label='Train Accuracy')\n",
        "    plt.plot(acc_valid_hist, label='Validation Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Sja1shT2wEjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_performance(loss_train_hist, loss_valid_hist, acc_train_hist, acc_valid_hist)"
      ],
      "metadata": {
        "id": "V2bj8OZVmauE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Discussing the Limitations of Accuracy**\n",
        "\n",
        "While accuracy is a commonly used metric, it can be misleading, especially in cases of class imbalance. In such scenarios, a model might predict the majority class for all instances, resulting in high accuracy but poor model performance in practical terms.\n",
        "\n",
        "**Why F1-Score?**\n",
        "The F1-score is a more robust metric in these cases as it balances precision and recall, providing a better measure of the classifier's performance, especially when the classes are imbalanced.\n",
        "\n",
        "**Importance of Confusion Matrix**\n",
        "The confusion matrix provides an in-depth view of the classifier's performance. It shows not just the overall accuracy but how the model performs on each individual class, revealing any biases or weaknesses in the model's predictions.\n"
      ],
      "metadata": {
        "id": "m9tlf-LHTdQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_f1_and_confusion_matrix(f1_train, f1_valid, cm_train, cm_valid):\n",
        "    # Plotting F1-Score\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.bar(['Train F1', 'Valid F1'], [f1_train, f1_valid], color=['blue', 'green'])\n",
        "    plt.title(\"F1-Score for Training and Validation\")\n",
        "\n",
        "    # Plotting Confusion Matrix\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.heatmap(cm_valid, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(\"Confusion Matrix for Validation\")\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "LNOVhwYlRnLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_f1_and_confusion_matrix(f1_train_hist[-1], f1_valid_hist[-1], cm_train_hist[-1], cm_valid_hist[-1])"
      ],
      "metadata": {
        "id": "6-wwYmvKR6nO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importance of ROC and AUC in Evaluation:**\n",
        "\n",
        "In medical imaging tasks like ours, accuracy isn't always the best performance metric due to potential class imbalances. Instead, we use ROC (Receiver Operating Characteristic) curves and AUC (Area Under the Curve) to provide a more nuanced view of our model's ability to distinguish between classes, ensuring a more reliable assessment of its diagnostic accuracy."
      ],
      "metadata": {
        "id": "I53RX1cie6Lp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "def evaluate_and_plot_roc(model, data_loader, device):\n",
        "    \"\"\"\n",
        "    Evaluates the model on the given data loader, calculates the ROC curve and AUC,\n",
        "    and plots the ROC curve.\n",
        "\n",
        "    Parameters:\n",
        "    model (torch.nn.Module): The trained model to evaluate.\n",
        "    data_loader (torch.utils.data.DataLoader): DataLoader for evaluation data.\n",
        "    device (torch.device): The device on which the model is.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Hint: Evaluate the model on the data loader, calculate the ROC curve and AUC using sklearn functions,\n",
        "    # and plot the ROC curve using matplotlib. Make sure to transfer data to the specified device.\n",
        "    # Your code here [30 score]\n"
      ],
      "metadata": {
        "id": "DzFaE2ZvsEsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_and_plot_roc(model, test_loader, device)"
      ],
      "metadata": {
        "id": "Rje-B83mqbl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Addressing Class Imbalance"
      ],
      "metadata": {
        "id": "jFF3XUCmvolh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "\n",
        "Class imbalance is a common challenge in medical data analysis. It happens when the number of examples in one class (usually the 'normal' category) is much larger than in another class (often representing a 'disease' condition). This imbalance can lead to models that are unfairly skewed towards the majority class, performing poorly in identifying the crucial, less represented class.\n",
        "\n",
        "\n",
        "**Why Address Class Imbalance?**\n",
        "\n",
        "In medical scenarios, the accuracy of detecting rare conditions (the minority class) is as important, if not more so, than identifying common ones. A model biased towards the majority class might overlook these critical minority cases, leading to potential misdiagnoses.\n",
        "\n",
        "\n",
        "To counter this, one approach is to use a weighted loss function during training. Follow the following hints to implement it.\n"
      ],
      "metadata": {
        "id": "XiDo5OfTZ2h1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hint: To address class imbalance, calculate class weights and apply them to the loss function.\n",
        "# 1. Calculate class distribution: Count the number of samples in each class.\n",
        "# 2. Calculate imbalance ratio: Compute the ratio of samples in the majority class to the minority class.\n",
        "# 3. Create class weights: Create a list or tensor containing the class weights based on the imbalance ratio.\n",
        "#    You may need to use the inverse of the ratio or other strategies to emphasize the minority class.\n",
        "# 4. Define the loss function: Use nn.BCEWithLogitsLoss and pass the class_weights as the 'weight' argument.\n",
        "\n",
        "# Your code here [15 score]\n"
      ],
      "metadata": {
        "id": "60QhkEXEvFaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model(model_name=\"swin_tiny_patch4_window7_224\",\n",
        "                        pretrained=True,\n",
        "                        num_classes=1,\n",
        "                        in_chans=n_channels)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "rsiIojzxwO5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "n_epochs = 100\n",
        "loss_train_hist = []\n",
        "loss_valid_hist = []\n",
        "acc_train_hist = []\n",
        "acc_valid_hist = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    model, loss_train, acc_train = train_one_epoch(model, train_loader, criterion, optimizer, epoch)\n",
        "    loss_valid, acc_valid = validate_one_epoch(model, val_loader, criterion)\n",
        "\n",
        "    loss_train_hist.append(loss_train)\n",
        "    loss_valid_hist.append(loss_valid)\n",
        "    acc_train_hist.append(acc_train)\n",
        "    acc_valid_hist.append(acc_valid)\n",
        "\n",
        "    print(f\"Epoch: {epoch+1}, Train Loss: {loss_train:.2f}, Train Acc: {acc_train:.2f}, Valid Loss: {loss_valid:.2f}, Valid Acc: {acc_valid:.2f}\")"
      ],
      "metadata": {
        "id": "khbq2Nqvv4QA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_performance(loss_train_hist, loss_valid_hist, acc_train_hist, acc_valid_hist)"
      ],
      "metadata": {
        "id": "LCNxvbR9wDG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_and_plot_roc(model, test_loader, device)"
      ],
      "metadata": {
        "id": "4H2e4-bYwDZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br><br>"
      ],
      "metadata": {
        "id": "VNmmF2972NDw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, you've already learned about addressing imbalanced data using a weighted loss. Now, let's explore an alternative method for tackling class imbalance in machine learning. Please describe and explain this alternative approach.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# Your answer here [15 score]\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "pfocGgmownQI"
      }
    }
  ]
}